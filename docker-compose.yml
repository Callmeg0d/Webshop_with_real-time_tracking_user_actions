services:
  db:
    container_name: "db"
    image: postgres:17
    environment:
      POSTGRES_USER: ${TEST_DB_USER}
      POSTGRES_PASSWORD: ${TEST_DB_PASS}
      POSTGRES_DB: ${TEST_DB_NAME}
    ports:
      - 5432:5432
    volumes:
      - postgresdata:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 5s
      retries: 5

  redis:
    container_name: "redis_db"
    image: redis:7
    ports:
      - 6379:6379
    volumes:
      - redisdata:/data

  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - 9092:9092
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk

  kafka-init:
    image: confluentinc/cp-kafka:latest
    depends_on:
      kafka:
        condition: service_started
    command: >
      sh -c "
      sleep 10 &&
      kafka-topics --delete --if-exists --bootstrap-server kafka:9092 --topic registration_confirmation 2>/dev/null || true &&
      kafka-topics --delete --if-exists --bootstrap-server kafka:9092 --topic order_confirmation 2>/dev/null || true &&
      kafka-topics --delete --if-exists --bootstrap-server kafka:9092 --topic clicks_topic 2>/dev/null || true &&
      kafka-topics --delete --if-exists --bootstrap-server kafka:9092 --topic page_views_topic 2>/dev/null || true &&
      kafka-topics --delete --if-exists --bootstrap-server kafka:9092 --topic scrolls_topic 2>/dev/null || true &&
      kafka-topics --delete --if-exists --bootstrap-server kafka:9092 --topic custom_events_topic 2>/dev/null || true &&
      kafka-topics --delete --if-exists --bootstrap-server kafka:9092 --topic other_events_topic 2>/dev/null || true &&
      sleep 2 &&
      kafka-topics --create --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic registration_confirmation &&
      kafka-topics --create --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic order_confirmation &&
      kafka-topics --create --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic clicks_topic &&
      kafka-topics --create --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic page_views_topic &&
      kafka-topics --create --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic scrolls_topic &&
      kafka-topics --create --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic custom_events_topic &&
      kafka-topics --create --bootstrap-server kafka:9092 --partitions 3 --replication-factor 1 --topic other_events_topic &&
      echo 'Topics created successfully with 3 partitions'
      "

  clickhouse:
    image: clickhouse/clickhouse-server:24.8
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    ulimits:
      nofile:
        soft: 262144
        hard: 262144
    volumes:
      - clickhousedata:/var/lib/clickhouse
      - ./app/clickhouse/init:/docker-entrypoint-initdb.d
    environment:
      - CLICKHOUSE_DB=analytics
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1

  clickhouse-init:
    image: clickhouse/clickhouse-server:24.8
    depends_on:
      clickhouse:
        condition: service_started
    volumes:
      - ./app/clickhouse/init:/docker-entrypoint-initdb.d
    entrypoint: >
      sh -c "
      sleep 5 &&
      clickhouse-client --host clickhouse --multiquery < /docker-entrypoint-initdb.d/001_create_events.sql &&
      echo 'ClickHouse schema initialized'
      "
    restart: "no"

  webshop:
    build:
      context: .
    image: webshop:latest
    container_name: webshop_app
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_started
      kafka:
        condition: service_started
      kafka-init:
        condition: service_completed_successfully
    command: sh -c "alembic upgrade head && gunicorn app.main:app --workers 3 --worker-class uvicorn.workers.UvicornWorker --bind=0.0.0.0:8000"
    ports:
      - 7777:8000

  prometheus:
    image: prom/prometheus:v2.43.0
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheusdata:/prometheus
    restart: unless-stopped
    ports:
      - 9090:9090

  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    volumes:
      - grafanadata:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
      - ./grafana/dashboards:/var/lib/grafana/dashboards
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_INSTALL_PLUGINS=vertamedia-clickhouse-datasource
    restart: unless-stopped
    ports:
      - 3000:3000
    depends_on:
      - clickhouse

volumes:
  postgresdata:
  redisdata:
  grafanadata:
  prometheusdata:
  clickhousedata: